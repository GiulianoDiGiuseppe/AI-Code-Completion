import pandas as pd
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from src.utils.configuration_utils import config
# Ensure NLTK resources are downloaded
nltk.download('punkt')

# Function to compute BLEU and ROUGE metrics
def compute_metrics(reference, generated_text):
    """
    Calculate BLEU and ROUGE metrics between the generated text and the reference.
    
    Args:
        reference (str): The reference text.
        generated_text (str): The text generated by the model.
        
    Returns:
        dict: A dictionary containing BLEU and ROUGE-L scores.
    """
    # Initialize the ROUGE scorer
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    
    # Tokenize the reference and generated text
    reference_tokens = nltk.word_tokenize(reference.lower())
    generated_tokens = nltk.word_tokenize(generated_text.lower())
    
    # Calculate BLEU score
    smoothing_function = SmoothingFunction().method1
    bleu_score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)
    
    # Calculate ROUGE-L score
    rouge_scores = scorer.score(reference, generated_text)
    rouge_l = rouge_scores['rougeL'].fmeasure

    # Return the BLEU and ROUGE-L scores
    return {
        'BLEU': bleu_score,
        'ROUGE-L': rouge_l
    }


if __name__ == "__main__":
    # Load the Excel file into a DataFrame
    df = pd.read_excel(config['input_excel_path'])

    # Create new columns in the DataFrame to save the metrics
    df['BLEU'] = None
    df['ROUGE-L'] = None

    # Iterate through the DataFrame and compute metrics
    for index, row in df.iterrows():
        label = row[config['label_column']]  # Reference text (label)
        generated_text = row[config['generated_column']]  # Text generated by the model
        
        # Compute metrics for the generated text against the label
        metrics = compute_metrics(label, generated_text)

        # Save the metrics scores in the DataFrame
        df.at[index, 'BLEU'] = metrics['BLEU']
        df.at[index, 'ROUGE-L'] = metrics['ROUGE-L']

    print(f"Columns in the DataFrame: {df.columns}")
    print(f"config metrics columns: {config['metrics_columns']}")

    # Group by 'taxonomy_column' and calculate the mean and count of the metrics
    # Correct the aggregation
    taxonomy_metrics = df.groupby(config['taxonomy_column']).agg(
        **{f'mean_{col}': (col, 'mean') for col in config['metrics_columns']},  # Calculate mean for each metric
        count=('Taxonomy', 'size')  # Count occurrences of each taxonomy
    ).reset_index()

    # Save the DataFrames with the prefix 'metrics'
    df.to_excel(config['output_metrics_path'], index=False)
    taxonomy_metrics.to_excel(config['output_taxonomy_metrics_path'], index=False)

    # Display the DataFrames (optional)
    print(df.head())
    print(taxonomy_metrics.head())